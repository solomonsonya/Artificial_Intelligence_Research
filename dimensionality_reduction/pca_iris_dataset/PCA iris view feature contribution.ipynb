{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b51b56-8473-430f-9589-41abc5cf7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as stat\n",
    "import scipy.stats as st\n",
    "import scipy as sp\n",
    "import os\n",
    "#import scikitplot as skplt\n",
    "import datetime\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sklearn\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "import statsmodels\n",
    "import warnings\n",
    "import io\n",
    "import inspect\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#scale the data via z-score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#preprocesing\n",
    "#learning and prediction algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.linear_model import Lasso, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder  # Optional for categorical labels\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# models\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import neural_network\n",
    "from sklearn import calibration\n",
    "\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# save and import trained models\n",
    "import pickle\n",
    "\n",
    "# Deep Learning\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from sklearn.datasets import make_classification\n",
    "\n",
    "# PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#importing  [Bagging]\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "\n",
    "#importing  [Boosting]\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "\n",
    "# model tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import make_scorer,mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_percentage_error, rand_score\n",
    "from sklearn.metrics import jaccard_score, dcg_score, consensus_score, d2_absolute_error_score\n",
    "from sklearn.metrics import d2_pinball_score, d2_tweedie_score, davies_bouldin_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#update system\n",
    "#!pip install --upgrade scikit-learn\n",
    "#!pip install xgboost\n",
    "#model tuning\n",
    "\n",
    "%matplotlib inline\n",
    "#to ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54066d-be23-40b1-8e4f-bed601dbce4b",
   "metadata": {},
   "source": [
    "# <b><u>Apply PCA for Feature Selection</b></u>\n",
    "\n",
    "<h2>All Features are still Used in PCA: Even though the plot or discussion might focus on a subset of principal components (like PC1, PC2, and PC3), all four original features are used to create these components. No feature is excluded; the contributions of each feature to each principal component are determined during the PCA process.</h2>\n",
    "\n",
    "# Extract coefficients of features (loadings) within each Principal Component \n",
    "<h2>These coefficients represent the amount of contribution of each feature to the respective PC. <br>\n",
    "Each value represents the weight (or importance) or the corresponding feature within the PC. <br>\n",
    "Large (absolute value) coefficients indicate the respective feature is important to the PC.<br>\n",
    "Positive and Negative signs indicate the (direction) of the contribution of each feature to each respective PC.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdb727c-322d-4676-a342-7885f5d838bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance ratio: [0.7296244541329988, 0.22850761786701757, 0.03668921889282881, 0.00517870910715476]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.377418</td>\n",
       "      <td>0.719566</td>\n",
       "      <td>-0.261286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_width</th>\n",
       "      <td>-0.269347</td>\n",
       "      <td>0.923296</td>\n",
       "      <td>-0.244382</td>\n",
       "      <td>0.123510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_length</th>\n",
       "      <td>0.580413</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>-0.142126</td>\n",
       "      <td>0.801449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_width</th>\n",
       "      <td>0.564857</td>\n",
       "      <td>0.066942</td>\n",
       "      <td>-0.634273</td>\n",
       "      <td>-0.523597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PC_1      PC_2      PC_3      PC_4\n",
       "sepal_length  0.521066  0.377418  0.719566 -0.261286\n",
       "sepal_width  -0.269347  0.923296 -0.244382  0.123510\n",
       "petal_length  0.580413  0.024492 -0.142126  0.801449\n",
       "petal_width   0.564857  0.066942 -0.634273 -0.523597"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "# Step 1: Load the Iris dataset\n",
    "###################################################\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Features\n",
    "X = iris.data\n",
    "\n",
    "# Target labels\n",
    "y = iris.target\n",
    "\n",
    "lst_attributes = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "# specify number of Principal Components - to start with, let n == number of features\n",
    "n = len(X[0])\n",
    "\n",
    "###################################################\n",
    "# Step 2: Standardize the data\n",
    "###################################################\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Step 3: Apply PCA to reduce to n components \n",
    "###################################################\n",
    "pca = PCA(n_components=n)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "###################################################\n",
    "# extract ratio explaining variance each PC provides\n",
    "###################################################\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "###################################################\n",
    "#notify\n",
    "###################################################\n",
    "print(f'explained_variance ratio: {list(explained_variance)}\\n')\n",
    "\n",
    "\n",
    "###################################################\n",
    "# create df showing magnitude of each feature to PC\n",
    "###################################################\n",
    "pca_coefficients = pca.components_\n",
    "df_pca_coefficient_loadings = pd.DataFrame(pca_coefficients.T, index=lst_attributes, columns=[f'PC_{i+1}' for i in range(len(lst_attributes))])\n",
    "\n",
    "df_pca_coefficient_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b76cc-a3b2-49c2-b191-f4d56e7f1698",
   "metadata": {},
   "source": [
    "<h2>The output DataFrame depicts each each feature (on each row).  Each column represents each principal component. <br>\n",
    "The values in the DataFrame are the coefficients (aka loadings) that indicate how much each original feature contributes to each principal component: </h2>\n",
    "\n",
    "* PC_1 is primarily equal contribution of sepal_length, petal_length, and petal_width <br>\n",
    "* PC_2 is primarily fully composed of sepal_width\n",
    "* PC_3 is primarily composed of sepal_length\n",
    "* PC_4 is primarily composed of petal_length\n",
    "\n",
    "<h1> We can use the coefficients in our PCA as a way to better understand which features are most influential in each principal component. <br> These can guide interpretation and feature selection.</h1>\n",
    "\n",
    "## View Normalized Feature Importance from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41ee294-f4da-421b-a97c-c962065a0d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <td>0.521066</td>\n",
       "      <td>0.377418</td>\n",
       "      <td>0.719566</td>\n",
       "      <td>0.261286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_width</th>\n",
       "      <td>0.269347</td>\n",
       "      <td>0.923296</td>\n",
       "      <td>0.244382</td>\n",
       "      <td>0.123510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_length</th>\n",
       "      <td>0.580413</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>0.142126</td>\n",
       "      <td>0.801449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_width</th>\n",
       "      <td>0.564857</td>\n",
       "      <td>0.066942</td>\n",
       "      <td>0.634273</td>\n",
       "      <td>0.523597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PC_1      PC_2      PC_3      PC_4\n",
       "sepal_length  0.521066  0.377418  0.719566  0.261286\n",
       "sepal_width   0.269347  0.923296  0.244382  0.123510\n",
       "petal_length  0.580413  0.024492  0.142126  0.801449\n",
       "petal_width   0.564857  0.066942  0.634273  0.523597"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca_coefficient_loadings = pd.DataFrame(np.abs(pca_coefficients.T), index=lst_attributes, columns=[f'PC_{i+1}' for i in range(len(lst_attributes))])\n",
    "df_pca_coefficient_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f5a0-0496-4037-b10a-342870ea91b3",
   "metadata": {},
   "source": [
    "Here's a breakdown of what the loadings tell you:\n",
    "\n",
    "Magnitude: The absolute value of a loading indicates the strength of the contribution.  A larger absolute value means the feature contributes more to that principal component.  A value close to zero means the feature contributes very little.\n",
    "\n",
    "Sign: The sign (positive or negative) indicates the direction of the relationship.\n",
    "\n",
    "A positive loading means the feature and the principal component are positively correlated. When the feature value increases, the principal component value tends to increase as well.   \n",
    "A negative loading means the feature and the principal component are negatively correlated. When the feature value increases, the principal component value tends to decrease.\n",
    "Interpretation: By looking at the loadings, you can understand what each principal component represents.  For example, if PC_1 has high positive loadings for petal_length and petal_width, you can infer that PC_1 represents something like \"overall flower size.\"\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a loading of 0.8 for petal_length on PC_1. This means:\n",
    "\n",
    "petal_length has a strong contribution to PC_1.\n",
    "petal_length and PC_1 are positively correlated. Longer petals tend to result in higher values for PC_1.Here's a breakdown of what the loadings tell you:\n",
    "\n",
    "Magnitude: The absolute value of a loading indicates the strength of the contribution.  A larger absolute value means the feature contributes more to that principal component.  A value close to zero means the feature contributes very little.\n",
    "\n",
    "Sign: The sign (positive or negative) indicates the direction of the relationship.\n",
    "\n",
    "A positive loading means the feature and the principal component are positively correlated. When the feature value increases, the principal component value tends to increase as well.   \n",
    "A negative loading means the feature and the principal component are negatively correlated. When the feature value increases, the principal component value tends to decrease.\n",
    "Interpretation: By looking at the loadings, you can understand what each principal component represents.  For example, if PC_1 has high positive loadings for petal_length and petal_width, you can infer that PC_1 represents something like \"overall flower size.\"\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a loading of 0.8 for petal_length on PC_1. This means:\n",
    "\n",
    "petal_length has a strong contribution to PC_1.\n",
    "petal_length and PC_1 are positively correlated. Longer petals tend to result in higher values for PC_1.\n",
    "If you have a loading of -0.6 for sepal_width on PC_2, this means:\n",
    "\n",
    "sepal_width has a moderately strong contribution to PC_2.\n",
    "sepal_width and PC_2 are negatively correlated. Wider sepals tend to result in lower values for PC_2.\n",
    "Key Point:  The loadings are essential for interpreting the meaning of the principal components. They tell you which original features are most influential in defining each PC.  This is one of the main reasons for performing PCA  to understand the underlying structure of your data and to potentially reduce its dimensionality while retaining the most important information.\n",
    "If you have a loading of -0.6 for sepal_width on PC_2, this means:\n",
    "\n",
    "sepal_width has a moderately strong contribution to PC_2.\n",
    "sepal_width and PC_2 are negatively correlated. Wider sepals tend to result in lower values for PC_2.\n",
    "Key Point:  The loadings are essential for interpreting the meaning of the principal components. They tell you which original features are most influential in defining each PC.  This is one of the main reasons for performing PCA  to understand the underlying structure of your data and to potentially reduce its dimensionality while retaining the most important information. - source: Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eee4169-455c-49cf-b071-b615acf8f696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>0.127706</td>\n",
       "      <td>-0.024168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>0.234609</td>\n",
       "      <td>-0.103007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "      <td>-0.044201</td>\n",
       "      <td>-0.028377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "      <td>-0.091290</td>\n",
       "      <td>0.065956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "      <td>-0.015738</td>\n",
       "      <td>0.035923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870503</td>\n",
       "      <td>0.386966</td>\n",
       "      <td>-0.256274</td>\n",
       "      <td>-0.389257</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.564580</td>\n",
       "      <td>-0.896687</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.521170</td>\n",
       "      <td>0.269069</td>\n",
       "      <td>-0.180178</td>\n",
       "      <td>-0.119171</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.372788</td>\n",
       "      <td>1.011254</td>\n",
       "      <td>-0.933395</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.960656</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>-0.528249</td>\n",
       "      <td>0.163078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PC_1      PC_2      PC_3      PC_4  Target\n",
       "0   -2.264703  0.480027  0.127706 -0.024168       0\n",
       "1   -2.080961 -0.674134  0.234609 -0.103007       0\n",
       "2   -2.364229 -0.341908 -0.044201 -0.028377       0\n",
       "3   -2.299384 -0.597395 -0.091290  0.065956       0\n",
       "4   -2.389842  0.646835 -0.015738  0.035923       0\n",
       "..        ...       ...       ...       ...     ...\n",
       "145  1.870503  0.386966 -0.256274 -0.389257       2\n",
       "146  1.564580 -0.896687  0.026371 -0.220192       2\n",
       "147  1.521170  0.269069 -0.180178 -0.119171       2\n",
       "148  1.372788  1.011254 -0.933395 -0.026129       2\n",
       "149  0.960656 -0.024332 -0.528249  0.163078       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## View how each df of PC's\n",
    "df_pca = pd.DataFrame(X_pca, columns=[f'PC_{i+1}' for i in range(len(lst_attributes))]) # use this loop vice hard coding columns=['PC_1', 'PC_2', 'PC_3']\n",
    "df_pca['Target'] = y\n",
    "df_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
