{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f6b932-0db0-422b-ac89-c6a9d0f183ca",
   "metadata": {},
   "source": [
    "Code augmented from:\n",
    "* https://www.databites.tech/p/hugging-face-use-cases-and-applications\n",
    "* https://github.com/rcalix1/TransferLearning/blob/main/HuggingfaceTransformers/2024/inClass_intro_HF.ipynb\n",
    "* https://github.com/rcalix1/TransferLearning/blob/main/HuggingfaceTransformers/2024/HelloWorldGTP2.ipynb\n",
    "* Professor Ricardo Calix, Purdue University Northwest \"Introduction to HuggingFace's Transformers module\", https://www.youtube.com/watch?v=D5fuMjkJf6k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010f6cf-c10e-4ae0-9e03-3a02c5095788",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a few usecases of transfer learning using language models to perform specified tasks. HuggingFace has a plethora of trained language models. With transfer learning, we can apply these pre-trained models to new domains and knowledge tasks\n",
    "\n",
    "<br>Notebook is by Solomon Sonya 0xSolomonSonya\n",
    "<br>Most code and data cells in this notebook have been augmented from ChatGPT, Copilot, Gemini, other Generative AI models, and online resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ded971-6fa5-434c-8e5f-b2b36eb778dc",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066558ed-63a8-4ef2-b11f-52cf9e0a9fa4",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret and generate human language. NLP bridges the gap between human communication and computer understanding.\n",
    "\n",
    "Large Language Models (LLMs) like BERT and GPT have significantly advanced the state of Natural Language Processing.  These models, trained on massive datasets of text and code, possess a deep understanding of language structure and semantics.\n",
    "\n",
    "Using these advanced LLMs, we can perform linguistic tasks such as:\n",
    "\n",
    "* Text classification: Categorizing text into predefined categories (e.g., spam detection, topic labeling).\n",
    "* Question answering: Providing answers to questions posed in natural language based on a given context or knowledge base.\n",
    "* Named entity recognition: Identifying and classifying named entities (e.g., people, organizations, locations) within text.\n",
    "* Sentiment analysis: Determining the emotional tone or attitude expressed in a piece of text (e.g., positive, negative, neutral).\n",
    "* Text prediction: Predicting the next word or sequence of words in a given text (e.g., autocomplete, predictive keyboard).\n",
    "* Summarization: Condensing a longer piece of text into a shorter, concise summary while preserving the key information.\n",
    "Text generation: Creating new text, such as stories, poems, articles, or code, based on a given prompt or context.\n",
    "\n",
    "- source: Gemini\n",
    "\n",
    "<br>BERT Models are based on Transformers and Encoders (given text it is able to classify the text and provide probabilities for next token (e.g., fill in the blank [MASK], and sentiment analysis). BERT models look in both directions (next word and previous word)\n",
    "<br>GPT Models are based on Transformers and Decoders (given a word, it calculates the probability for the next word, Feed forward in a single direction, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7ae5f6-4102-480b-88e1-9bfc14fc4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SentencePiece\n",
    "#!pip install torch\n",
    "#!pip install datasets\n",
    "#!pip install --upgrade --force-reinstall pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ac4731-664b-42e5-bd2f-40655b94a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06058e0c-f8cc-45f6-b930-62f097f2627b",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80789c5-2bad-4298-a1f9-bda56dcdce1e",
   "metadata": {},
   "source": [
    "# Text Prediction - Viewing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5889bcfd-5ce1-40be-a5ff-f961c3794584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.9141,  1.2348, -0.6978,  ...,  0.3912, -1.2460,  1.1479],\n",
      "         [ 0.8212, -0.0263,  0.6866,  ..., -0.2959,  1.2790, -0.6806],\n",
      "         [-0.1347,  0.1948, -0.3194,  ..., -0.3486,  2.2491, -1.0724],\n",
      "         ...,\n",
      "         [-0.2334, -0.3141,  0.0913,  ...,  1.6683,  0.2309,  0.5323],\n",
      "         [ 0.3367,  0.5625, -0.2566,  ..., -0.4797,  1.4007,  0.3033],\n",
      "         [ 0.0500,  0.1402, -0.0665,  ..., -0.0931,  0.1098,  0.2176]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4812, -0.4942,  0.9253, -0.9148, -0.7215, -0.9468,  0.4822, -0.4793,\n",
      "          0.5641, -0.9999,  0.8975,  0.3981, -0.6010, -0.9836, -0.9098, -0.4541,\n",
      "          0.4660,  0.4578,  0.9904, -0.4105, -0.8281, -0.9952,  0.9985,  0.9632,\n",
      "          0.6430, -0.4528,  0.5644, -0.9889, -0.9986, -0.4758, -1.0000,  0.5501,\n",
      "          0.5359,  0.4877,  0.5160, -0.4009,  0.5471,  0.9606, -0.5019,  0.4890,\n",
      "          0.4574, -0.9673, -0.8559,  0.4776,  0.4977,  0.4819,  0.9059, -0.9794,\n",
      "          0.9061, -0.4567, -0.4263, -0.4362, -0.3848, -0.9580, -0.6937,  0.5155,\n",
      "         -0.5469, -0.3934,  0.9999, -0.8037,  0.4914, -0.5321,  0.5714, -0.1361,\n",
      "         -0.5284,  0.4606,  0.4478,  0.9984, -0.4273,  0.9885,  0.4804,  0.5081,\n",
      "          0.4840, -0.6182,  0.9870,  0.5384,  0.8005,  0.4453,  0.6452, -0.9978,\n",
      "          0.9071,  0.5445, -0.5233,  0.5361, -0.8695, -0.9887,  0.4163, -0.9998,\n",
      "          0.5044,  0.9791,  0.4713,  0.5424, -0.4777, -1.0000,  0.4359, -0.5050,\n",
      "         -0.9995,  0.5232,  0.4333, -0.5102,  0.7135, -0.4462,  0.5304, -0.5488,\n",
      "         -0.4403, -0.4151,  0.4589,  0.8431,  0.4645,  0.9805, -0.9884, -0.4689,\n",
      "          0.3984,  0.9781, -0.4932,  0.2651, -0.5190,  0.9203, -0.9988,  0.4829,\n",
      "          0.4449, -0.2621,  0.4948, -0.3488,  0.5014,  0.1285,  0.3701, -0.9315,\n",
      "          0.9721, -0.9464,  0.3154,  0.4502, -0.9992, -0.5514,  0.4991,  0.9437,\n",
      "         -0.4276, -0.4896, -0.4926,  0.4953,  0.9666, -0.4906,  0.4162, -0.5717,\n",
      "         -0.4831,  0.4486,  0.7751, -0.3670,  0.4207,  0.9746, -0.9516,  0.9914,\n",
      "          0.7104, -0.8178, -0.9967,  0.1914,  0.9958, -0.8893,  0.8237, -0.4998,\n",
      "         -0.4228, -0.9111, -0.9981, -0.5061, -0.9957, -0.4982,  0.9988,  0.0661,\n",
      "          0.9975, -0.9978, -0.4196,  0.4395, -0.4263,  0.9831,  0.4799,  0.5216,\n",
      "          0.4277,  0.7911, -0.4653, -0.3786,  0.9847, -0.9997,  0.4317,  0.5400,\n",
      "          0.9327,  0.5134,  0.5341, -0.9545,  0.4726, -0.8312, -0.4568, -0.8688,\n",
      "          0.5143,  0.7942,  1.0000, -0.5471,  0.9073, -0.9919,  0.9595, -0.9962,\n",
      "         -0.5128,  0.8234,  0.9914,  0.4657,  0.5142,  0.9201, -0.9520, -0.9923,\n",
      "         -0.9985, -0.4917, -0.9994,  0.9773, -0.9863,  0.5137, -0.9943,  0.9946,\n",
      "          0.9927, -0.4679,  0.9991, -0.5139,  0.6066,  0.5145, -0.9998,  0.9802,\n",
      "          0.3536,  0.4446, -0.2635, -0.4728,  0.9860, -0.9954, -0.9680, -0.6951,\n",
      "          0.5123, -0.9985, -0.7973, -0.5903,  0.5387,  0.5219,  0.5560, -0.9997,\n",
      "          0.9998,  0.5346, -0.4345, -0.5293, -0.2685,  0.9999, -0.6990, -0.9665,\n",
      "         -0.4705,  0.9991,  0.9530,  0.5603,  0.5783, -0.5065,  0.8486, -0.4200,\n",
      "         -0.9972, -0.9903, -0.9878,  0.4767, -0.9891,  0.3618, -0.3651, -0.9997,\n",
      "         -0.5185,  0.9520,  0.9926,  0.9947,  0.4755, -0.6097,  0.4191, -0.4676,\n",
      "          0.9712, -0.6083,  0.9507, -0.3787, -0.9781,  0.5092,  0.5914,  0.4021,\n",
      "         -0.5310, -0.9104,  0.5530, -0.5070,  0.9985, -0.8313,  0.9983, -0.9602,\n",
      "         -0.9992,  0.5368, -0.5352, -0.5129,  0.9985, -0.4535, -0.9989, -0.9994,\n",
      "         -0.1595,  0.9814,  0.7921, -0.9781,  0.4891, -0.5357, -0.5120,  0.9940,\n",
      "          0.5093, -0.4548, -0.3628,  0.4288,  0.4651,  0.6154, -0.8723, -0.5073,\n",
      "         -0.5096, -0.9880, -0.5607, -0.5799, -0.3825, -0.7239,  0.9979,  0.9931,\n",
      "         -0.4899, -0.5781,  0.9957, -0.9996,  0.5670, -0.9998,  0.7084, -0.9992,\n",
      "         -0.9974, -0.1907, -0.8166, -0.4915, -0.5561, -0.9826,  0.4972, -0.9960,\n",
      "          0.4201, -0.5353,  0.9885,  0.9543, -0.9997, -0.3849, -0.8339,  0.3997,\n",
      "          0.5520,  0.4516,  0.4173, -0.1098,  0.5175, -0.9956,  0.5089,  0.8994,\n",
      "         -0.5171,  0.9406, -0.5215,  0.4977, -0.9478, -0.7985, -0.4604,  0.9839,\n",
      "          0.9948, -0.8181, -0.4167,  0.4920, -0.9375,  0.9903, -0.9929,  0.9871,\n",
      "         -0.9939, -0.5332, -0.9706,  0.9993,  0.9756,  0.6313, -0.5616, -0.9592,\n",
      "         -0.9954,  0.5259, -0.5081, -0.4183, -0.4716,  0.9616,  0.4627,  0.9641,\n",
      "         -0.8887,  0.3423,  0.3560,  0.9601, -0.9780,  0.9868, -0.9410, -0.5023,\n",
      "          0.8186,  1.0000, -0.5255,  0.4595, -0.9931, -0.9724, -0.4427,  0.4798,\n",
      "          0.9970, -0.4470, -0.9529,  0.9622,  0.9883, -0.9968,  0.5014,  0.9987,\n",
      "          0.8914,  0.5446,  0.5447,  0.8936,  0.9524,  0.4694,  0.9365, -0.4459,\n",
      "          0.9989, -0.9960, -0.9992,  0.9832, -0.4486,  0.9951, -0.8516,  0.4591,\n",
      "         -0.3502,  0.5143, -0.5097, -0.5217,  0.4206,  0.9617,  0.5469,  0.9904,\n",
      "          0.3979, -0.9972, -0.9999,  0.3793,  0.6041,  0.5186, -0.4743,  0.9181,\n",
      "         -0.4630, -0.5739, -0.5772, -0.5313,  0.4605, -0.9992,  0.9998, -0.9945,\n",
      "          0.9933, -0.4538,  0.9243,  0.8515,  0.9826, -0.5634, -1.0000, -0.5552,\n",
      "         -0.9883,  0.5220, -0.4386, -0.9772, -0.4825,  0.9364, -0.4648,  0.9950,\n",
      "          0.6132, -0.6188, -0.9931,  1.0000, -0.8617, -0.9993,  0.4509,  0.4498,\n",
      "          0.7382,  0.4829,  0.3894,  0.4451, -0.8226, -0.5129,  0.9899, -0.9533,\n",
      "          0.1717, -0.9796,  0.9773, -0.5465, -0.2703,  0.7276,  0.9867,  0.9978,\n",
      "         -0.1596, -1.0000, -0.9820, -0.9994, -0.9819,  0.4335, -0.9817,  0.8390,\n",
      "         -0.5225, -0.3947,  0.9432,  0.7694, -0.4345, -0.4143, -0.9854, -0.8539,\n",
      "         -0.4791, -0.9348,  0.5326, -0.9995, -0.6925, -0.9209, -0.9939, -0.7272,\n",
      "         -0.9929,  0.4319, -0.9997,  0.7253, -0.5020,  0.2987, -0.4848,  0.4882,\n",
      "          0.4437, -0.4338,  0.4052,  0.9743, -0.5017,  0.9999,  0.9969,  0.9041,\n",
      "          0.5257, -0.9231, -0.4564,  0.9960, -0.3468, -0.8374,  0.9491, -0.9179,\n",
      "          0.7029, -0.9951,  0.9969, -0.9392, -0.9442, -0.9900, -0.4778, -0.9999,\n",
      "         -0.9981, -0.9996,  0.5381,  0.9931, -0.9885,  0.9997,  0.5824, -0.8437,\n",
      "          0.9980,  0.5544, -0.9766,  0.4343, -0.5105,  0.9998, -0.4117,  0.9630,\n",
      "         -0.4637, -0.9879,  0.9724,  0.9820, -0.4356, -0.4533, -0.4684, -0.9397,\n",
      "          0.5025, -0.4483,  0.5532,  0.9997, -0.9047,  0.4655, -0.5129,  0.4781,\n",
      "         -0.9957,  0.4258, -0.5719, -0.3489, -0.9893, -0.9630,  0.9975,  0.4544,\n",
      "          0.4820,  0.4546,  0.5263, -0.3288,  0.9995, -0.9999, -0.4315,  0.4545,\n",
      "         -0.7971,  0.8346, -0.4011, -0.5754,  0.4051,  0.9853, -0.4056,  0.9849,\n",
      "         -0.0608,  0.5632,  0.5487,  0.4320,  0.4427,  0.9983,  0.5232, -0.9086,\n",
      "         -0.4073, -0.4822, -0.4569,  0.9969,  0.9992, -0.9683, -0.3317, -0.4170,\n",
      "         -0.9982,  0.9873,  0.7930,  0.9992,  0.9990,  0.5903, -0.3549, -0.9850,\n",
      "          0.9999,  0.9302, -0.5118,  0.6333,  0.6011, -0.4870,  0.0013,  0.9635,\n",
      "         -0.9626, -0.0280,  0.5559,  0.9934,  0.9884, -0.4563,  0.8350, -0.9996,\n",
      "         -0.5051,  0.9158, -0.9975,  0.5086,  0.9969, -0.9999, -0.4523, -0.5032,\n",
      "          0.9915,  0.9692,  0.4418,  0.4305, -0.3390, -0.5191,  0.9995,  0.3874,\n",
      "         -0.5219,  0.9965, -0.4720, -0.3672,  0.5321,  0.5678,  0.9988,  0.9788,\n",
      "         -0.9818, -0.4253,  0.4703, -0.5107,  0.9891, -0.9998, -0.4365,  0.3198,\n",
      "         -0.7706,  0.4034,  0.8853,  0.5538, -0.5940, -0.9992,  0.4496,  0.9629,\n",
      "          0.4767,  0.9866,  0.4650, -0.5522,  0.1219,  0.7697, -0.1911,  0.8254,\n",
      "         -0.9983,  0.4368, -0.8841,  0.4625, -0.5496, -0.9766, -0.5913,  0.4953,\n",
      "          0.9946,  0.5264,  0.5281,  0.1787, -0.5494,  0.5427,  0.4597,  0.4300,\n",
      "         -0.4699, -0.5545, -0.5246, -0.9972,  0.5180,  0.4863,  0.2184,  0.5626,\n",
      "         -0.3905, -0.9307,  0.4523, -0.4472,  0.6155, -0.9612, -0.9998, -0.5463,\n",
      "         -0.9719, -0.3969,  0.5179, -0.5664,  0.5253, -0.5090, -1.0000, -0.4578,\n",
      "          0.3702, -0.9415,  0.5010, -0.9503,  0.4097,  1.0000,  1.0000, -0.9908,\n",
      "          0.4213, -0.9995, -0.4854, -0.5027, -1.0000,  0.5082,  1.0000,  0.5373,\n",
      "          0.3803, -0.9952, -0.9207,  0.9979, -0.9454, -0.5440,  0.4744, -0.4198,\n",
      "         -0.9994,  0.2882,  0.5208,  0.5472,  0.5255, -0.9959, -0.5706,  0.7471,\n",
      "         -0.9974,  0.3864,  0.9897, -0.4431,  0.4385,  0.1105, -0.9889,  0.5481]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n",
      "CPU times: user 390 ms, sys: 0 ns, total: 390 ms\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# code in this section is augmented from source: \n",
    "#        https://github.com/rcalix1/TransferLearning/blob/main/HuggingfaceTransformers/2024/inClass_intro_HF.ipynb\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# we will be using the tokenizer specific to the BERT model\n",
    "# each model is optimized for specific task(s) and then we can transfer the pretrained model and apply to our domain with fine tuning\n",
    "# the tokenizer transforms text to numbers\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "model     = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "\n",
    "text = \"this dog is very happy.\"\n",
    "\n",
    "# output is all numbers to be decoded\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb921691-8f87-41ff-9447-4db3915a1dec",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0996b-05c6-4dc5-af92-cd63e257c964",
   "metadata": {},
   "source": [
    "# Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0748d810-9bc9-48fb-a194-b0268f4eca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.281032</td>\n",
       "      <td>10901</td>\n",
       "      <td>cute</td>\n",
       "      <td>the cat is so cute .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094896</td>\n",
       "      <td>26354</td>\n",
       "      <td>adorable</td>\n",
       "      <td>the cat is so adorable .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042963</td>\n",
       "      <td>1700</td>\n",
       "      <td>happy</td>\n",
       "      <td>the cat is so happy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040976</td>\n",
       "      <td>5066</td>\n",
       "      <td>funny</td>\n",
       "      <td>the cat is so funny .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024234</td>\n",
       "      <td>28803</td>\n",
       "      <td>affectionate</td>\n",
       "      <td>the cat is so affectionate .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token     token_str                      sequence\n",
       "0  0.281032  10901          cute          the cat is so cute .\n",
       "1  0.094896  26354      adorable      the cat is so adorable .\n",
       "2  0.042963   1700         happy         the cat is so happy .\n",
       "3  0.040976   5066         funny         the cat is so funny .\n",
       "4  0.024234  28803  affectionate  the cat is so affectionate ."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fillmask = pipeline('fill-mask', model='albert-base-v2')\n",
    "\n",
    "# specify where a word should be filled in\n",
    "res = pd.DataFrame(fillmask(\"The cat is so [MASK] .\"))\n",
    "\n",
    "# the result is a dataframe providing probability of next word based on pretrained model's embeddings\n",
    "# token of greatest probability is what we'll use as the most likely response\n",
    "# cute seems to be more likely here\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "541a7531-60c3-4714-9271-e8e307a229d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031417</td>\n",
       "      <td>27668</td>\n",
       "      <td>figurative</td>\n",
       "      <td>el chapo is a figurative person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028689</td>\n",
       "      <td>18496</td>\n",
       "      <td>franciscan</td>\n",
       "      <td>el chapo is a franciscan person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025276</td>\n",
       "      <td>9650</td>\n",
       "      <td>dominican</td>\n",
       "      <td>el chapo is a dominican person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022960</td>\n",
       "      <td>19210</td>\n",
       "      <td>moroccan</td>\n",
       "      <td>el chapo is a moroccan person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017772</td>\n",
       "      <td>14484</td>\n",
       "      <td>basque</td>\n",
       "      <td>el chapo is a basque person.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token   token_str                          sequence\n",
       "0  0.031417  27668  figurative  el chapo is a figurative person.\n",
       "1  0.028689  18496  franciscan  el chapo is a franciscan person.\n",
       "2  0.025276   9650   dominican   el chapo is a dominican person.\n",
       "3  0.022960  19210    moroccan    el chapo is a moroccan person.\n",
       "4  0.017772  14484      basque      el chapo is a basque person."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = pd.DataFrame(fillmask(\"El chapo is a  [MASK] person.\"))\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b5f31a3-0abe-4c14-b91d-3f85b26ce1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059892</td>\n",
       "      <td>5934</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>anna is a wonderful person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057768</td>\n",
       "      <td>5066</td>\n",
       "      <td>funny</td>\n",
       "      <td>anna is a funny person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047016</td>\n",
       "      <td>254</td>\n",
       "      <td>good</td>\n",
       "      <td>anna is a good person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046513</td>\n",
       "      <td>8601</td>\n",
       "      <td>lovely</td>\n",
       "      <td>anna is a lovely person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038441</td>\n",
       "      <td>2210</td>\n",
       "      <td>nice</td>\n",
       "      <td>anna is a nice person.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token  token_str                     sequence\n",
       "0  0.059892   5934  wonderful  anna is a wonderful person.\n",
       "1  0.057768   5066      funny      anna is a funny person.\n",
       "2  0.047016    254       good       anna is a good person.\n",
       "3  0.046513   8601     lovely     anna is a lovely person.\n",
       "4  0.038441   2210       nice       anna is a nice person."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = pd.DataFrame(fillmask(\"Anna is a  [MASK] person.\"))\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c75c79fa-00f4-427c-be68-3faedbf96bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090354</td>\n",
       "      <td>5934</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>michael is a wonderful person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056277</td>\n",
       "      <td>254</td>\n",
       "      <td>good</td>\n",
       "      <td>michael is a good person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054211</td>\n",
       "      <td>5066</td>\n",
       "      <td>funny</td>\n",
       "      <td>michael is a funny person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051298</td>\n",
       "      <td>374</td>\n",
       "      <td>great</td>\n",
       "      <td>michael is a great person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049223</td>\n",
       "      <td>2210</td>\n",
       "      <td>nice</td>\n",
       "      <td>michael is a nice person.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token  token_str                        sequence\n",
       "0  0.090354   5934  wonderful  michael is a wonderful person.\n",
       "1  0.056277    254       good       michael is a good person.\n",
       "2  0.054211   5066      funny      michael is a funny person.\n",
       "3  0.051298    374      great      michael is a great person.\n",
       "4  0.049223   2210       nice       michael is a nice person."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = pd.DataFrame(fillmask(\"Michael is a  [MASK] person.\"))\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9424ab8d-fbb8-4ff3-b257-49c4feda1063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416072</td>\n",
       "      <td>39</td>\n",
       "      <td>she</td>\n",
       "      <td>the nurse is examining the patient. she is wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.227184</td>\n",
       "      <td>28153</td>\n",
       "      <td>joyah</td>\n",
       "      <td>the nurse is examining the patient. joyah is w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140536</td>\n",
       "      <td>29833</td>\n",
       "      <td>evalle</td>\n",
       "      <td>the nurse is examining the patient. evalle is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007875</td>\n",
       "      <td>24</td>\n",
       "      <td>he</td>\n",
       "      <td>the nurse is examining the patient. he is writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003686</td>\n",
       "      <td>23512</td>\n",
       "      <td>jaenelle</td>\n",
       "      <td>the nurse is examining the patient. jaenelle i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str  \\\n",
       "0  0.416072     39       she   \n",
       "1  0.227184  28153     joyah   \n",
       "2  0.140536  29833    evalle   \n",
       "3  0.007875     24        he   \n",
       "4  0.003686  23512  jaenelle   \n",
       "\n",
       "                                            sequence  \n",
       "0  the nurse is examining the patient. she is wri...  \n",
       "1  the nurse is examining the patient. joyah is w...  \n",
       "2  the nurse is examining the patient. evalle is ...  \n",
       "3  the nurse is examining the patient. he is writ...  \n",
       "4  the nurse is examining the patient. jaenelle i...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example of bias in the pretrained dataset\n",
    "res3 = pd.DataFrame(fillmask(\"The nurse is examining the patient.  [MASK] is writing down notes.\"))\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecc708d5-c03e-4852-a29f-269e7bb2f32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210027</td>\n",
       "      <td>24</td>\n",
       "      <td>he</td>\n",
       "      <td>the doctor is examining the patient. he is wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.208616</td>\n",
       "      <td>39</td>\n",
       "      <td>she</td>\n",
       "      <td>the doctor is examining the patient. she is wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108847</td>\n",
       "      <td>28153</td>\n",
       "      <td>joyah</td>\n",
       "      <td>the doctor is examining the patient. joyah is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071828</td>\n",
       "      <td>29833</td>\n",
       "      <td>evalle</td>\n",
       "      <td>the doctor is examining the patient. evalle is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010178</td>\n",
       "      <td>1687</td>\n",
       "      <td>doctor</td>\n",
       "      <td>the doctor is examining the patient. doctor is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str  \\\n",
       "0  0.210027     24        he   \n",
       "1  0.208616     39       she   \n",
       "2  0.108847  28153     joyah   \n",
       "3  0.071828  29833    evalle   \n",
       "4  0.010178   1687    doctor   \n",
       "\n",
       "                                            sequence  \n",
       "0  the doctor is examining the patient. he is wri...  \n",
       "1  the doctor is examining the patient. she is wr...  \n",
       "2  the doctor is examining the patient. joyah is ...  \n",
       "3  the doctor is examining the patient. evalle is...  \n",
       "4  the doctor is examining the patient. doctor is...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example of bias in the pretrained dataset\n",
    "res3 = pd.DataFrame(fillmask(\"The doctor is examining the patient.  [MASK] is writing down notes.\"))\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cf8c4cc-c2f8-4074-835b-3582a63d32d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341382</td>\n",
       "      <td>24</td>\n",
       "      <td>he</td>\n",
       "      <td>the pilot is flying the plane. he is doing a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144531</td>\n",
       "      <td>29833</td>\n",
       "      <td>evalle</td>\n",
       "      <td>the pilot is flying the plane. evalle is doing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067653</td>\n",
       "      <td>39</td>\n",
       "      <td>she</td>\n",
       "      <td>the pilot is flying the plane. she is doing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055728</td>\n",
       "      <td>28153</td>\n",
       "      <td>joyah</td>\n",
       "      <td>the pilot is flying the plane. joyah is doing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012015</td>\n",
       "      <td>1266</td>\n",
       "      <td>everyone</td>\n",
       "      <td>the pilot is flying the plane. everyone is doi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  token token_str  \\\n",
       "0  0.341382     24        he   \n",
       "1  0.144531  29833    evalle   \n",
       "2  0.067653     39       she   \n",
       "3  0.055728  28153     joyah   \n",
       "4  0.012015   1266  everyone   \n",
       "\n",
       "                                            sequence  \n",
       "0  the pilot is flying the plane. he is doing a g...  \n",
       "1  the pilot is flying the plane. evalle is doing...  \n",
       "2  the pilot is flying the plane. she is doing a ...  \n",
       "3  the pilot is flying the plane. joyah is doing ...  \n",
       "4  the pilot is flying the plane. everyone is doi...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = pd.DataFrame(fillmask(\"The pilot is flying the plane.  [MASK] is doing a great job!\"))\n",
    "res3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f79931-2b0d-46d1-bf2f-9f34832af374",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce0ad6-0980-4fee-acad-d4611611f08d",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08587d-dd34-4473-95e5-433ae97a2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import AutoModel, pipeline, BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "model     = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "nlp = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d7412-d357-4a18-a80b-c3a3bcb452dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Aviation is the activities surrounding mechanical flight and the aircraft industry. Aircraft includes fixed-wing and rotary-wing types, \n",
    "morphable wings, wing-less lifting bodies, as well as lighter-than-air craft such as hot air balloons and airships.\n",
    "Aviation began in the 18th century with the development of the hot air balloon, an apparatus capable of atmospheric displacement through buoyancy. \n",
    "Some of the most significant advancements in aviation technology came with the controlled gliding flying of Otto Lilienthal in 1896; then a large \n",
    "step in significance came with the construction of the first powered airplane by the Wright brothers in the early 1900s. Since that time, aviation \n",
    "has been technologically revolutionized by the introduction of the jet which permitted a major form of transport throughout the world.\n",
    "'''\n",
    "\n",
    "q = nlp(text)\n",
    "\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff319fb1-7073-4820-93de-50917a833ecb",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1f3aa-4ff8-41eb-a27b-34c39c532962",
   "metadata": {},
   "source": [
    "# Text Classification - Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c189f0-60e7-4aaf-90fe-234bbad49bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This example is ['travel', 'cooking', 'dancing'].\n",
      "CPU times: user 3.9 s, sys: 74.3 ms, total: 3.98 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "sequence = \"I am going to france\" \n",
    "\n",
    "label = ['travel', 'cooking', 'dancing']\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "premise    = sequence\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "print(hypothesis)\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b72109e-61e7-4fe7-83a4-014b60286e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   100,   524,   164,     7,  6664,  2389,     2,     2,   713,\n",
       "          1246,    16, 47052, 28881,  3934,   128, 35190,   154,  3934,   128,\n",
       "           417,  7710,   108,  8174,     2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b46fcdf-6f18-47ce-b856-24ba127b7e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0327,  1.3776,  0.6796]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = nli_model(x.to(device))[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78c590-b49e-4f06-a38d-8f52b07572b3",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfda728b-b397-4865-89ae-a459b81d07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b59bad5-7b07-4f6e-bada-e98214bdcf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('we love you')          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2c6191-f007-4708-a9e8-9243a361d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9988259673118591}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('we hate you')          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ff033f-681a-4955-b1ee-2e8052970566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9996534585952759}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('my cat kind of likes you')          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27bdac9-d5bd-4d96-bf14-aef3fe0977ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9846875071525574}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('i am going to the store')          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b34d53-5957-4df3-8bad-5c7447f1146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.8937287926673889}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('bacon')          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81e2dfef-54db-4569-8e33-54af78864aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9635980725288391}]\n"
     ]
    }
   ],
   "source": [
    "print(     pipeline('sentiment-analysis')('the')          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ffc058-d5a7-42fc-82ea-4c59edb6cda2",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75797eb7-4c28-441f-b402-57c91120bfee",
   "metadata": {},
   "source": [
    "# Text Classification - Text Categorization/Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c66fef6-9f7c-49bf-b020-5ccf48ad023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 0 ns, total: 3 μs\n",
      "Wall time: 7.87 μs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c50c1ba-0af3-479b-9e08-97ca6f9c4838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'i like that tobacco rates have fallen', 'labels': ['smoking', 'pets', 'cars'], 'scores': [0.9401766657829285, 0.03462902829051018, 0.025194326415657997]}\n"
     ]
    }
   ],
   "source": [
    "sequence_to_classify = \"i like that tobacco rates have fallen\"\n",
    "\n",
    "candidate_labels = ['cars', 'smoking', 'pets']\n",
    "\n",
    "result = classifier(sequence_to_classify, candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06fa68-3231-4a0f-96c8-0a1c235f695e",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75aec6-a84f-4e75-ac8f-5737a64b12e2",
   "metadata": {},
   "source": [
    "# Text Classification - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690f7c3-eafd-43dd-b653-f665c683d032",
   "metadata": {},
   "source": [
    "Text classification is a core task in NLP that involves assigning one or more categories to a given input text. It has a wide range of applications, including spam detection, sentiment analysis, topic labeling, and more. - source: https://www.databites.tech/p/hugging-face-use-cases-and-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0bdb645-9012-4b2f-9e9b-f601b8a85293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9797219038009644}]\n",
      "CPU times: user 323 ms, sys: 94.2 ms, total: 418 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using transfer learning, we will classify text as positive, negative, or neutral\n",
    "\n",
    "# We import the pipeline module from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# We load the pre-trained text classification model.\n",
    "classifier = pipeline(\"text-classification\",model='lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "\n",
    "# Input to be classified\n",
    "input = \"I truly love the hugging face library!\"\n",
    "\n",
    "# Perform classification\n",
    "output = classifier(input)\n",
    "\n",
    "# Observe the result\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8a54b-d4fe-46ba-8a92-257fe73c68b4",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47593f8-a20f-473e-8cbe-ccb96b4131f1",
   "metadata": {},
   "source": [
    "# Text generation - Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542e5d2-d82c-4bca-9329-7973e736a5e0",
   "metadata": {},
   "source": [
    "Many of you are probably familiar with tools like ChatGPT, Claude, or Google Gemini—platforms that generate text based on an input prompt. This process, known as text generation, is an area of NLP where a model creates human-like responses from a given input.\n",
    "Text generation has a wide range of uses, from building chatbot conversations to generating creative content. The core idea is to train a model on massive text datasets, allowing it to learn the patterns, styles, and structures of natural language. As you might expect, the most resource-intensive part is training the model itself. - source: https://www.databites.tech/p/hugging-face-use-cases-and-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09de6872-cd1a-469a-af5f-2609b91957a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although AI is just starting today, I wonder if the next step will be one based on a few very specific algorithms. You've probably already said:\n",
      "\n",
      "1. Go to the machine learning section, and create your portfolio of deep learning algorithms.\n",
      "CPU times: user 2.62 s, sys: 243 ms, total: 2.86 s\n",
      "Wall time: 4.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We import the pipeline module from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# We load the pre-trained text generation model.\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Input to be classified\n",
    "prompt = \"Although AI is just starting today,\"\n",
    "\n",
    "# Generate the new text\n",
    "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
    "\n",
    "# Observe the result\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfcf27-67e1-418a-b3ff-37f852cc8c84",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e598a-de54-4565-b3e8-814f54517746",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef0d65-ba78-4418-aa3f-5fc467d8dafd",
   "metadata": {},
   "source": [
    "<b>Question answering</b>, commonly referred to as QA, is a field in NLP focused on building systems that automatically answer questions posed by humans in natural language.\n",
    "\n",
    "QA systems are widely used in various applications, such as virtual assistants, customer support, and information retrieval systems.\n",
    "\n",
    "QA systems can be broadly categorized into two types:\n",
    "\n",
    "* <b>Open-domain QA:</b> Answers questions based on a broad range of knowledge, often sourced from the internet or large databases.\n",
    "* <b>Closed-domain QA:</b> Focuses on a specific domain, like medicine or law, and answers questions from a limited dataset.\n",
    "\n",
    "<br> These systems typically use a combination of natural language understanding to interpret the question and information retrieval to find relevant answers. - source: https://www.databites.tech/p/hugging-face-use-cases-and-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba595c16-18e1-43ea-8045-0a48480892f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.954908013343811, 'start': 121, 'end': 130, 'answer': '2,140,526'}\n",
      "CPU times: user 260 ms, sys: 22.5 ms, total: 283 ms\n",
      "Wall time: 898 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We import the pipeline module from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# We load the question-answering text generation model.\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "# Context to ask about\n",
    "context = \"\"\"Paris is the capital and most populous city of France. The city has an area of 105 square kilometers and a population of 2,140,526 residents.\"\"\"\n",
    "\n",
    "# Question to perform to the model\n",
    "question = \"What is the population of Paris?\"\n",
    "\n",
    "# Now we get the answer\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74082a8-3e66-4d9f-bfb2-7ae773424512",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6435b-5c73-4394-977f-d48e51feb613",
   "metadata": {},
   "source": [
    "# Translation - Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f97cc7-dd16-40c3-b69d-9e0729cd54c9",
   "metadata": {},
   "source": [
    "- source: https://www.databites.tech/p/hugging-face-use-cases-and-applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84093104-da1b-4723-bd2e-5743eccd860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dies ist ein großer Tag für die Wissenschaft!\n",
      "CPU times: user 2.52 s, sys: 206 ms, total: 2.72 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We import the pipeline module from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the translation pipeline for English to Spanish\n",
    "translator = pipeline('translation_en_to_de')\n",
    "\n",
    "# Text to translate from English to Spanish\n",
    "text_to_translate = \"This is a great day for science!\"\n",
    "\n",
    "# Perform the translation\n",
    "translation = translator(text_to_translate, max_length=40)\n",
    "\n",
    "# Print the translated text\n",
    "print(translation[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb8f1e-1b41-4048-8b94-3b1328e8f6f3",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4059af-d9a1-40c6-b920-84557d3de208",
   "metadata": {},
   "source": [
    "# GPT2\n",
    "NOTE: There are better pretrained models! Be sure to experiement with them. This one is useful as a starter model to experiement with GPT (Decoders). There are better trained models (these will likely be significantly larger memory resources to operate on your machine.)\n",
    "\n",
    "NOTE: Halucinations are probably more prevalent in the earlier GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de3229-10d0-4230-9a27-f9ddb315fcae",
   "metadata": {},
   "source": [
    "Purpose of this is to experiement with Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35289d8d-34f9-4504-8225-3fcf1053283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# tokenizer allows us to map numbers to words\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "import json\n",
    "\n",
    "# variables for text generation\n",
    "seed                   = 42\n",
    "max_length             = 150\n",
    "num_return_sequences   = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989f9c82-5b52-40e3-907c-7c0d2b17d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to view output from LLM in a more standardized format\n",
    "def generate_examples( generator, prompt_list ):    \n",
    "    set_seed(seed)    \n",
    "    examples = []\n",
    "    \n",
    "    for prompt in prompt_list: \n",
    "        \n",
    "        result = generator(\n",
    "                   prompt, \n",
    "                   max_length           = max_length, \n",
    "                   num_return_sequences = num_return_sequences )\n",
    "        \n",
    "        example = {'prompt': prompt}\n",
    "        \n",
    "        for i, res in enumerate( result ):            \n",
    "            ## answer = res['generated_text'].lstrip().removeprefix( prompt ).strip()\n",
    "            answer    = res['generated_text'].lstrip().strip()            \n",
    "            example[f'answer{ i + 1 }'] = answer #given a prompt, take an answer\n",
    "            \n",
    "        examples.append(example)        \n",
    "        ## print(examples)\n",
    "        print( json.dumps( example, indent = 2) )\n",
    "        \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe349358-04a2-4514-8125-231171b73052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# use a pipeline to allow us to configure our LLM easily\n",
    "# pipeline will help us configure the machine with the appropriate package specified\n",
    "# The GPT model will continue to generate text until max length is reached at this point, it will return the response back to us\n",
    "model_name           = 'gpt2'\n",
    "model_gpt_generator  = pipeline('text-generation', model=model_name ) # we can specify device for gpu or cpu\n",
    "tokenizer              = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb2481d-c3fc-4c52-986c-21c0ecdc606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"Are electric vehicles more efficient to petroleum vehicles?\",\n",
      "  \"answer1\": \"Are electric vehicles more efficient to petroleum vehicles?\",\n",
      "  \"answer2\": \"Are electric vehicles more efficient to petroleum vehicles?\\n\\nElectric vehicles are considered more environmentally responsible vehicles because they are more environmentally efficient compared to gasoline.\\n\\nIs there still enough data on gasoline-electric use to find an exact answer?\\n\\nNo, there is not. No reliable data will be available till the next year.\\n\\nWhat are the key criteria to determine if a car is not environmentally responsible?\\n\\nNo, the only vehicles on the map that need to be considered in determining if a vehicle is environmentally responsible when they have been equipped with a safety-rated battery include cars with EPA rated batteries.\\n\\nDo you see any specific standards or guidelines that you are interested in pursuing?\\n\\nWe have a variety of guidelines within\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# provide a list of questions to supply to the LLM\n",
    "list_to_answer = [\"Are electric vehicles more efficient to petroleum vehicles?\"]\n",
    "\n",
    "# supply the question to GPT and it will generate a sequence of words most closely related to our question\n",
    "say_something = generate_examples( model_gpt_generator, list_to_answer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b8b3f1-3336-489f-86f0-70d44196cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"What evidence is present to state global warming is a threat to civilization?\",\n",
      "  \"answer1\": \"What evidence is present to state global warming is a threat to civilization? Are we witnessing some sort of imminent apocalypse? I see a very large proportion of scientists who, under those conditions, do not believe that such a thing has been possible.\\\"\\n\\nA spokesman for the U.N. Climate Change Commission suggested that the panel could not immediately answer these questions. \\\"We will be looking into these questions once the U.N. has finished with its report and is prepared to release its findings,\\\" a spokesperson for the department of environment said in a statement. \\\"We believe it is prudent for the scientific community to have a more comprehensive review of the evidence before making an official statement.\\\"\",\n",
      "  \"answer2\": \"What evidence is present to state global warming is a threat to civilization?\\n\\nThe first step is to recognize the threat it presents, to recognize that the climate system is not completely changing, even in the absence of widespread adaptation, and to make the appropriate policy choices. This leads to better understanding and the better understanding and recognition of the scientific facts and the reasons for the increase in extreme weather events, weather extremes and extreme heat waves.\\n\\nThe second step is to recognize and actively promote comprehensive measures to limit the rate and volume of heat waves, extreme heat waves and other extreme weather events from the developing world. These steps could include monitoring the sea level rise and ocean circulation, the presence of storms and cyclones, taking measures to prevent their occurrence\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# provide a list of questions to supply to the LLM\n",
    "list_to_answer = [\"What evidence is present to state global warming is a threat to civilization?\"]\n",
    "\n",
    "# supply the question to GPT and it will generate a sequence of words most closely related to our question\n",
    "say_something = generate_examples( model_gpt_generator, list_to_answer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302dfa5-a07e-4882-99e5-ae8ea884078c",
   "metadata": {},
   "source": [
    "<hr style=\"height:75px;color:#000;background-color:#000;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
