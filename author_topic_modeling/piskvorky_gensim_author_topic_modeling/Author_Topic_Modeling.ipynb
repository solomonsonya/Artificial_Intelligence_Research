{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1218db-c87b-4173-a24c-1098fffd8636",
   "metadata": {},
   "source": [
    "# Duplication of gensim/docs/notebooks /atmodel_tutorial.ipynb \n",
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb\n",
    "\n",
    "All code is from the original author at the link above. I'm recreating here to have a working copy such that I can transfer this concept to other domains in cybersecurity.\n",
    "\n",
    "Special Shout out to Professor Berkay Celik @ZBerkayCelik directing me to this notebook. \n",
    "\n",
    "Solomon Sonya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce2719-f92a-4077-b72a-e8d46a0e14d6",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d60409-ac48-4d8c-8a73-336535e3d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and execute these if needed\n",
    "#!pip install gensim\n",
    "#!pip install smart_open\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4760684f-cc9b-4aad-a54b-4187a14573dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7586e82-51b3-4367-bc95-7c38be7633b8",
   "metadata": {},
   "source": [
    "# The author-topic model: LDA with metadata\n",
    "\n",
    "In this tutorial, you will learn how to use the author-topic model in Gensim. We will apply it to a corpus consisting of scientific papers, to get insight about the authors of the papers.\n",
    "\n",
    "The author-topic model is an extension of Latent Dirichlet Allocation (LDA), that allows us to learn topic representations of authors in a corpus. The model can be applied to any kinds of labels on documents, such as tags on posts on the web. The model can be used as a novel way of data exploration, as features in machine learning pipelines, for author (or tag) prediction, or to simply leverage your topic model with existing metadata.\n",
    "\n",
    "To learn about the theoretical side of the author-topic model, see [Rosen-Zvi and co-authors 2004](https://mimno.infosci.cornell.edu/info6150/readings/398.pdf), for example. A report on the algorithm used in the Gensim implementation will be available soon.\n",
    "\n",
    "Naturally, familiarity with topic modelling, LDA and Gensim is assumed in this tutorial. If you are not familiar with either LDA, or its Gensim implementation, I would recommend starting there. Consider some of these resources:\n",
    "* Gentle introduction to the LDA model: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "* Gensim's LDA API documentation: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* Topic modelling in Gensim: https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "* [Pre-processing and training LDA](lda_training_tips.ipynb)\n",
    "\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> To run this tutorial on your own, install Jupyter, Gensim, SpaCy, Scikit-Learn, Bokeh and Pandas, e.g. using pip:\n",
    ">\n",
    "> `pip install jupyter gensim spacy sklearn bokeh pandas`\n",
    ">\n",
    "> Note that you need to download some data for SpaCy using `python -m spacy.en.download`.\n",
    ">\n",
    "> Download the notebook at https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks/atmodel_tutorial.ipynb.\n",
    "\n",
    "In this tutorial, we will learn how to prepare data for the model, how to train it, and how to explore the resulting representation in different ways. We will inspect the topic representation of some well known authors like Geoffrey Hinton and Yann LeCun, and compare authors by plotting them in reduced dimensionality and performing similarity queries.\n",
    "\n",
    "## Analyzing scientific papers\n",
    "\n",
    "The data we will be using consists of scientific papers about machine learning, from the Neural Information Processing Systems conference (NIPS). It is the same dataset used in the [Pre-processing and training LDA](lda_training_tips.ipynb) tutorial, mentioned earlier.\n",
    "\n",
    "We will be performing qualitative analysis of the model, and at times this will require an understanding of the subject matter of the data. If you try running this tutorial on your own, consider applying it on a dataset with subject matter that you are familiar with. For example, try one of the [StackExchange datadump datasets](https://archive.org/details/stackexchange).\n",
    "\n",
    "You can download the data from Sam Roweis' website (http://www.cs.nyu.edu/~roweis/data.html). Or just run the cell below, and it will be downloaded and extracted into your `tmp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa207f32-ac5e-4300-aeb7-7c370e7b632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-21 02:05:47--  http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving www.cs.nyu.edu (www.cs.nyu.edu)... 216.165.22.203\n",
      "Connecting to www.cs.nyu.edu (www.cs.nyu.edu)|216.165.22.203|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz [following]\n",
      "--2024-11-21 02:05:47--  https://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Connecting to www.cs.nyu.edu (www.cs.nyu.edu)|216.165.22.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz [following]\n",
      "--2024-11-21 02:05:47--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving cs.nyu.edu (cs.nyu.edu)... 216.165.22.203\n",
      "Connecting to cs.nyu.edu (cs.nyu.edu)|216.165.22.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 12851423 (12M) [application/x-gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "100%[======================================>] 12,851,423  55.1MB/s   in 0.2s   \n",
      "\n",
      "2024-11-21 02:05:48 (55.1 MB/s) - written to stdout [12851423/12851423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O - 'http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz' > ./nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7d4e1e-7857-4016-b437-4a66c61ac6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 557 ms, sys: 390 ms, total: 947 ms\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tarfile\n",
    "\n",
    "filename = './nips12raw_str602.tgz'\n",
    "tar = tarfile.open(filename, 'r:gz')\n",
    "for item in tar:\n",
    "    tar.extract(item, path='./tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f70c1-1d92-4386-bf23-bba8dcd154d6",
   "metadata": {},
   "source": [
    "In the following sections we will load the data, pre-process it, train the model, and explore the results using some of the implementation's functionality. Feel free to skip the loading and pre-processing for now, if you are familiar with the process.\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "In the cell below, we crawl the folders and files in the dataset, and read the files into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8c75b-ca53-4847-936d-aa22b965dab0",
   "metadata": {},
   "source": [
    "Construct a mapping from author names to document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3247e1a9-e1d1-4869-a58d-907a771075dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 576, 1208]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from smart_open import smart_open\n",
    "\n",
    "# Folder containing all NIPS papers.\n",
    "data_dir = '/tmp/nipstxt/'  # Set this path to the data on your machine.\n",
    "\n",
    "# Folders containing individual NIPS papers.\n",
    "yrs = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "dirs = ['nips' + yr for yr in yrs]\n",
    "\n",
    "# Get all document texts and their corresponding IDs.\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for yr_dir in dirs:\n",
    "    files = os.listdir(data_dir + yr_dir)  # List of filenames.\n",
    "    for filen in files:\n",
    "        # Get document ID.\n",
    "        (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start and end of the ID.\n",
    "        doc_ids.append(yr_dir[4:] + '_' + str(int(filen[idx1:idx2])))\n",
    "        \n",
    "        # Read document text.\n",
    "        # Note: ignoring characters that cause encoding errors.\n",
    "        with smart_open(data_dir + yr_dir + '/' + filen, 'rb', encoding='utf-8', errors='ignore') as fid:\n",
    "            txt = fid.read()\n",
    "            \n",
    "        # Replace any whitespace (newline, tabs, etc.) with a single space.\n",
    "        txt = re.sub('\\s', ' ', txt)\n",
    "        \n",
    "        docs.append(txt)\n",
    "\n",
    "# Initialize the author2doc dictionary.\n",
    "author2doc = dict()\n",
    "i = 0\n",
    "\n",
    "# Now process the author-document mappings for each year.\n",
    "for yr in yrs:\n",
    "    # The files \"a00.txt\" and so on contain the author-document mappings.\n",
    "    filename = data_dir + 'idx/a' + yr + '.txt'\n",
    "    with smart_open(filename, 'rb', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            # Each line corresponds to one author.\n",
    "            contents = re.split(',', line)\n",
    "            author_name = (contents[1] + contents[0]).strip()  # Swap last name and first name\n",
    "            author_name = re.sub('\\s', '', author_name)  # Remove any whitespace to standardize the name.\n",
    "            \n",
    "            # Get document IDs for the author.\n",
    "            ids = [c.strip() for c in contents[2:]]\n",
    "            if not author2doc.get(author_name):\n",
    "                # This is a new author, initialize their list.\n",
    "                author2doc[author_name] = []\n",
    "                i += 1\n",
    "            \n",
    "            # Add document IDs to the author's list.\n",
    "            author2doc[author_name].extend([yr + '_' + id for id in ids])\n",
    "\n",
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS dataset to an integer ID.\n",
    "doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "\n",
    "# Replace NIPS IDs by integer IDs in the author2doc mapping.\n",
    "for a, a_doc_ids in author2doc.items():\n",
    "    for idx, doc_id in enumerate(a_doc_ids):\n",
    "        author2doc[a][idx] = doc_id_dict.get(doc_id, -1)  # Handle any missing mapping safely.\n",
    "\n",
    "print(author2doc['YaserS.Abu-Mostafa'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb45902-33e4-47a5-a9a5-fea65ded3449",
   "metadata": {},
   "source": [
    "### Pre-processing text\n",
    "\n",
    "The text will be pre-processed using the following steps:\n",
    "* Tokenize text.\n",
    "* Replace all whitespace by single spaces.\n",
    "* Remove all punctuation and numbers.\n",
    "* Remove stopwords.\n",
    "* Lemmatize words.\n",
    "* Add multi-word named entities.\n",
    "* Add frequent bigrams.\n",
    "* Remove frequent and rare words.\n",
    "\n",
    "A lot of the heavy lifting will be done by the great package, Spacy. Spacy markets itself as \"industrial-strength natural language processing\", is fast, enables multiprocessing, and is easy to use. First, let's import it and load the NLP pipline in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4c389f-2fae-41f2-a5db-93a1e2c3dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/ssonya/.local/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ssonya/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/ssonya/.local/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ssonya/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ssonya/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ssonya/.local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/ssonya/.local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/ssonya/.local/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "complete!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print('\\ncomplete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20877d8-4448-4aee-a1f5-5eaef52ad665",
   "metadata": {},
   "source": [
    "In the code below, Spacy takes care of tokenization, removing non-alphabetic characters, removal of stopwords, lemmatization and named entity recognition.\n",
    "\n",
    "Note that we only keep named entities that consist of more than one word, as single word named entities are already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149c8ceb-ed62-414c-9863-880602d37ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.7 s, sys: 1.35 s, total: 31.1 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_process=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "\n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    processed_tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than one word.\n",
    "    processed_tokens.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    processed_docs.append(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ada927c-1741-457e-8cf0-e3520ac6e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe708fa-dc02-4595-b50b-84c968e512b2",
   "metadata": {},
   "source": [
    "Below, we use a Gensim model to add bigrams. Note that this achieves the same goal as named entity recognition, that is, finding adjacent words that have some particular significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c914bf42-aa9b-4bca-9141-d72b21ecf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c8954-adfa-4e2b-b167-71e1442493cd",
   "metadata": {},
   "source": [
    "Now we are ready to construct a dictionary, as our vocabulary is finalized. We then remove common words (occurring $> 50\\%$ of the time), and rare words (occur $< 20$ times in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf99f338-64b9-4457-a92e-0e40cafc1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a9189-d6b6-4dab-92e7-6bc845006780",
   "metadata": {},
   "source": [
    "We produce the vectorized representation of the documents, to supply the author-topic model with, by computing the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45b4ed2-418f-4c62-983e-5d72b40e7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64518f-1384-4284-a1ab-343910e69288",
   "metadata": {},
   "source": [
    "Let's inspect the dimensionality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36fe8c02-f0e6-4204-a954-f431f581ba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 2479\n",
      "Number of unique tokens: 7742\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f4260-015d-49b3-a773-5209500fa24f",
   "metadata": {},
   "source": [
    "### Train and use model\n",
    "\n",
    "We train the author-topic model on the data prepared in the previous sections. \n",
    "\n",
    "The interface to the author-topic model is very similar to that of LDA in Gensim. In addition to a corpus, ID to word mapping (`id2word`) and number of topics (`num_topics`), the author-topic model requires either an author to document ID mapping (`author2doc`), or the reverse (`doc2author`).\n",
    "\n",
    "Below, we have also (this can be skipped for now):\n",
    "* Increased the number of `passes` over the dataset (to improve the convergence of the optimization problem).\n",
    "* Decreased the number of `iterations` over each document (related to the above).\n",
    "* Specified the mini-batch size (`chunksize`) (primarily to speed up training).\n",
    "* Turned off bound evaluation (`eval_every`) (as it takes a long time to compute).\n",
    "* Turned on automatic learning of the `alpha` and `eta` priors (to improve the convergence of the optimization problem).\n",
    "* Set the random state (`random_state`) of the random number generator (to make these experiments reproducible).\n",
    "\n",
    "We load the model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f18ca22-8f3d-4224-b740-6bafaae03756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 579 ms, sys: 1.89 ms, total: 581 ms\n",
      "Wall time: 580 ms\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7072e6-cd7f-4aaf-8b24-32be14b54dfd",
   "metadata": {},
   "source": [
    "If you believe your model hasn't converged, you can continue training using `model.update()`. If you have additional documents and/or authors call `model.update(corpus, author2doc)`.\n",
    "\n",
    "Before we explore the model, let's try to improve upon it. To do this, we will train several models with different random initializations, by giving different seeds for the random number generator (`random_state`). We evaluate the topic coherence of the model using the [top_topics](https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.top_topics) method, and pick the model with the highest topic coherence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c918dbfd-fa93-460e-8dd2-1d76a3b9f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 19s, sys: 112 ms, total: 3min 19s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                    author2doc=author2doc, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b695f02-a77e-4b51-88a0-691d7d912937",
   "metadata": {},
   "source": [
    "Choose the model with the highest topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d37cdebb-d9b8-4726-ab2a-afdfe56540a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic coherence: -1.116e+01\n"
     ]
    }
   ],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe297ff-5b9d-4c6f-ba5a-7c239bd8efcd",
   "metadata": {},
   "source": [
    "We save the model, to avoid having to train it again, and also show how to load it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c71c3-94a9-4596-a6d0-7fe128f5e095",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb837cd1-5a1b-419c-aee4-874a6a06eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "#model.save('/tmp/model.atmodel')\n",
    "model.save('./author_topic_model.atmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5d358-c943-46a7-8df5-bb58b389cc90",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "547bf3e2-6457-442d-9ccc-6b8d1c6ecd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "#model = AuthorTopicModel.load('./author_topic_model.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4093a47b-5485-4f3c-9cdb-a61cc5a65784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sample', 0.007349484811500737),\n",
       " ('prior', 0.007177555444560171),\n",
       " ('mixture', 0.006986627184039146),\n",
       " ('likelihood', 0.006111061035622703),\n",
       " ('density', 0.005805155661023329),\n",
       " ('Gaussian', 0.005609518286166241),\n",
       " ('component', 0.005462723558404004),\n",
       " ('matrix', 0.00544793860285116),\n",
       " ('class', 0.005367647069688331),\n",
       " ('kernel', 0.0050730405358949205)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic(0)\n",
    "\n",
    "# solomon sonya observation: this topic values look a little different from the original example at https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb\n",
    "# so I would expect the topic_labels to not align up exactly - but the over concept is not lost in how we can manually label our topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c7874-1873-43bd-8479-d946737caedf",
   "metadata": {},
   "source": [
    "### Explore author-topic representation\n",
    "\n",
    "Now that we have trained a model, we can start exploring the authors and the topics.\n",
    "\n",
    "First, let's simply print the most important words in the topics. Below we have printed topic 0. As we can see, each topic is associated with a set of words, and each word has a probability of being expressed under that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a02523-fc7c-4f53-91a3-26b0e9b7757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Circuits', 'Neuroscience', 'Numerical optimization', 'Object recognition', \\\n",
    "               'Math/general', 'Robotics', 'Character recognition', \\\n",
    "                'Reinforcement learning', 'Speech recognition', 'Bayesian modelling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05793311-e06d-4a17-8bbd-84d6e3b095b2",
   "metadata": {},
   "source": [
    "Rather than just calling `model.show_topics(num_topics=10)`, we format the output a bit so it is easier to get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "642a56d8-6f98-4c77-ac13-2aee0b9a004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Circuits\n",
      "Words: sample prior mixture likelihood density Gaussian component matrix class kernel \n",
      "\n",
      "Label: Neuroscience\n",
      "Words: image object layer distance map position representation control view recognition \n",
      "\n",
      "Label: Numerical optimization\n",
      "Words: action policy control Learning reinforcement rule optimal environment robot goal \n",
      "\n",
      "Label: Object recognition\n",
      "Words: cell neuron stimulus response activity spike visual synaptic field cortex \n",
      "\n",
      "Label: Math/general\n",
      "Words: word classifier recognition class classification speech layer hide sequence net \n",
      "\n",
      "Label: Robotics\n",
      "Words: solution rule matrix N dynamic p memory constraint energy neuron \n",
      "\n",
      "Label: Character recognition\n",
      "Words: image signal component filter source representation visual layer matrix response \n",
      "\n",
      "Label: Reinforcement learning\n",
      "Words: noise prediction approximation optimal gradient generalization nonlinear variance N w \n",
      "\n",
      "Label: Speech recognition\n",
      "Words: let bound class node f m threshold Theorem bind p \n",
      "\n",
      "Label: Bayesian modelling\n",
      "Words: circuit neuron chip voltage signal analog VLSI channel noise implement \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in model.show_topics(num_topics=10):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in model.show_topic(topic[0]):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7463049-c09b-4593-b50b-f68a26b52757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.8612464711302933), (4, 0.13854770253430757)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['YannLeCun']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022ff6d-ae23-4705-bbe7-feeb23dd583b",
   "metadata": {},
   "source": [
    "Let's print the top topics of some authors. First, we make a function to help us do this more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7e7c6c-1873-40ed-959a-85c52179500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d0140-a253-4f6a-a8c3-86b4bb14de42",
   "metadata": {},
   "source": [
    "Below, we print some high profile researchers and inspect them. Three of these, Yann LeCun, Geoffrey E. Hinton and Christof Koch, are spot on. \n",
    "\n",
    "Terrence J. Sejnowski's results are surprising, however. He is a neuroscientist, so we would expect him to get the \"neuroscience\" label. This may indicate that Sejnowski works with the neuroscience aspects of visual perception, or perhaps that we have labeled the topic incorrectly, or perhaps that this topic simply is not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8e0c751-db4d-4927-8c0e-20a50aa43b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YannLeCun\n",
      "Docs: [115, 291, 329, 548, 541, 431, 641, 614, 822, 735, 1449]\n",
      "Topics:\n",
      "[('Neuroscience', 0.8612464711302933), ('Math/general', 0.13854770253430757)]\n"
     ]
    }
   ],
   "source": [
    "show_author('YannLeCun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4654b48-8c29-4924-a784-54fe78c37c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GeoffreyE.Hinton\n",
      "Docs: [67, 115, 216, 211, 227, 449, 433, 496, 653, 831, 746, 903, 898, 1357, 1732, 1713]\n",
      "Topics:\n",
      "[('Circuits', 0.8543582157289198),\n",
      " ('Numerical optimization', 0.1455366375698559)]\n"
     ]
    }
   ],
   "source": [
    "show_author('GeoffreyE.Hinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f09194a2-4b9d-4dd7-92f2-8baf30db8aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TerrenceJ.Sejnowski\n",
      "Docs: [519, 522, 505, 490, 696, 655, 607, 698, 742, 717, 947, 927, 908, 850, 936, 895, 916, 1273, 1182, 1276, 1158, 1226, 1285, 1241, 1344, 1292, 1541, 1477, 1536, 1645, 1674]\n",
      "Topics:\n",
      "[('Character recognition', 0.9999253818141417)]\n"
     ]
    }
   ],
   "source": [
    "show_author('TerrenceJ.Sejnowski')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc810dbe-d858-402e-b180-55b0e239eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChristofKoch\n",
      "Docs: [43, 271, 212, 263, 307, 408, 417, 352, 462, 482, 650, 645, 744, 825, 754, 970, 1239, 1212, 1236, 1524, 1585, 1557, 1576, 1594]\n",
      "Topics:\n",
      "[('Object recognition', 0.5457872569656348),\n",
      " ('Bayesian modelling', 0.4541231087446922)]\n"
     ]
    }
   ],
   "source": [
    "show_author('ChristofKoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098f76f-56fb-4279-813a-977ba6cfee79",
   "metadata": {},
   "source": [
    "#### Simple model evaluation methods\n",
    "\n",
    "We can compute the per-word bound, which is a measure of the model's predictive performance (you could also say that it is the reconstruction error).\n",
    "\n",
    "To do that, we need the `doc2author` dictionary, which we can build automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca769438-7620-4a37-9a29-80de6eca64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c4992-4607-4db9-8368-479bef136d4d",
   "metadata": {},
   "source": [
    "Now let's evaluate the per-word bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3012afbd-62ec-4071-92d9-c960c3e12e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.8380149286571745\n"
     ]
    }
   ],
   "source": [
    "# Compute the per-word bound.\n",
    "# Number of words in corpus.\n",
    "corpus_words = sum(cnt for document in model.corpus for _, cnt in document)\n",
    "\n",
    "# Compute bound and divide by number of words.\n",
    "perwordbound = model.bound(model.corpus, author2doc=model.author2doc, \\\n",
    "                           doc2author=model.doc2author) / corpus_words\n",
    "print(perwordbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114bb7f-8aa6-4d92-99b4-a80f70766ea9",
   "metadata": {},
   "source": [
    "We can evaluate the quality of the topics by computing the topic coherence, as in the LDA class. Use this to e.g. find out which of the topics are poor quality, or as a metric for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15c08df8-e049-4bc8-ae67-809f9f248507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 147 ms, sys: 5 µs, total: 147 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%time top_topics = model.top_topics(model.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853abd0-c1f9-4466-86e8-6b5ec8f6b6aa",
   "metadata": {},
   "source": [
    "#### Plotting the authors\n",
    "\n",
    "Now we're going to produce the kind of pacific archipelago looking plot below. The goal of this plot is to give you a way to explore the author-topic representation in an intuitive manner.\n",
    "\n",
    "We take all the author-topic distributions (stored in `model.state.gamma`) and embed them in a 2D space. To do this, we reduce the dimensionality of this data using t-SNE. \n",
    "\n",
    "t-SNE is a method that attempts to reduce the dimensionality of a dataset, while maintaining the distances between the points. That means that if two authors are close together in the plot below, then their topic distributions are similar.\n",
    "\n",
    "In the cell below, we transform the author-topic representation into the t-SNE space. You can increase the `smallest_author` value if you do not want to view all the authors with few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d82ad143-ebfa-4ae1-a26a-a8eeebd650c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 1.95 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3993702-d9f3-46c3-b062-a25774f440a9",
   "metadata": {},
   "source": [
    "We are now ready to make the plot.\n",
    "\n",
    "Note that if you run this notebook yourself, you will see a different graph. The random initialization of the model will be different, and the result will thus be different to some degree. You may find an entirely different representation of the data, or it may show the same interpretation slightly differently.\n",
    "\n",
    "If you can't see the plot, you are probably viewing this tutorial in a Jupyter Notebook. View it in an nbviewer instead at http://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc3b4dca-feb6-4a8c-9c80-fba433cfa572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"f1968e5d-7661-415e-8708-0f86e00f13b2\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"f1968e5d-7661-415e-8708-0f86e00f13b2\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"f1968e5d-7661-415e-8708-0f86e00f13b2\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"f1968e5d-7661-415e-8708-0f86e00f13b2\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"f1968e5d-7661-415e-8708-0f86e00f13b2\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d836dc19-efd0-4163-9d6d-e398d7761163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ssonya/NLP/mlp_malware_language_processing/Author-Topic Modeling/bokeh_tsne_plot.html'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource, output_file, save\n",
    "\n",
    "# Generate data for the plot\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "    data=dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        author_names=author_names,\n",
    "        author_sizes=author_sizes,\n",
    "        radii=radii,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a fresh hover tool\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"Author\", \"@author_names\"),\n",
    "        (\"Size\", \"@author_sizes\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a new figure\n",
    "p = figure(\n",
    "    tools=['pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'lasso_select'],  # Re-create tools here\n",
    "    title=\"TSNE Scatter Plot of Authors\",\n",
    "    x_axis_label=\"TSNE Dimension 1\",\n",
    "    y_axis_label=\"TSNE Dimension 2\",\n",
    ")\n",
    "\n",
    "# Add hover tool separately to avoid sharing issues\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Scatter plot\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "\n",
    "# Save to an HTML file\n",
    "output_file(\"bokeh_tsne_plot.html\")  # Specify output file\n",
    "save(p)  # Save the plot to an HTML file\n",
    "\n",
    "# Show the plot in the browser\n",
    "#show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba8f6da7-e507-458f-844a-4d043b45d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"bokeh_tsne_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2ab75977e810>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Adjust the width and height as needed\n",
    "IFrame(src='bokeh_tsne_plot.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d247050-2c18-4c19-9fdb-dc787772b223",
   "metadata": {},
   "source": [
    "The circles in the plot above are individual authors, and their sizes represent the number of documents attributed to the corresponding author. Hovering your mouse over the circles will tell you the name of the authors and their sizes. Large clusters of authors tend to reflect some overlap in interest. \n",
    "\n",
    "We see that the model tends to put duplicate authors close together. For example, Terrence J. Sejnowki and T. J. Sejnowski are the same person, and their vectors end up in the same place (see about $(-10, -10)$ in the plot).\n",
    "\n",
    "At about $(-15, -10)$ we have a cluster of neuroscientists like Christof Koch and James M. Bower. \n",
    "\n",
    "As discussed earlier, the \"object recognition\" topic was assigned to Sejnowski. If we get the topics of the other authors in Sejnoski's neighborhood, like Peter Dayan, we also get this same topic. Furthermore, we see that this cluster is close to the \"neuroscience\" cluster discussed above, which is further indication that this topic is about visual perception in the brain.\n",
    "\n",
    "Other clusters include a reinforcement learning cluster at about $(-5, 8)$, and a Bayesian modelling cluster at about $(8, -12)$.\n",
    "\n",
    "#### Similarity queries\n",
    "\n",
    "In this section, we are going to set up a system that takes the name of an author and yields the authors that are most similar. This functionality can be used as a component in an information retrieval (i.e. a search engine of some kind), or in an author prediction system, i.e. a system that takes an unlabelled document and predicts the author(s) that wrote it.\n",
    "\n",
    "We simply need to search for the closest vector in the author-topic space. In this sense, the approach is similar to the t-SNE plot above.\n",
    "\n",
    "Below we illustrate a similarity query using a built-in similarity framework in Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "492c5d91-3f09-4044-8253-82b8a8d296ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(model.id2author.values())])\n",
    "\n",
    "# Get similarities to some author.\n",
    "author_name = 'YannLeCun'\n",
    "sims = index[model[author_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fac75-cc66-4e59-9c3d-1bde24979e4e",
   "metadata": {},
   "source": [
    "However, this framework uses the cosine distance, but we want to use the Hellinger distance. The Hellinger distance is a natural way of measuring the distance (i.e. dis-similarity) between two probability distributions. Its discrete version is defined as\n",
    "$$\n",
    "H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^K (\\sqrt{p_i} - \\sqrt{q_i})^2},\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are both topic distributions for two different authors. We define the similarity as\n",
    "$$\n",
    "S(p, q) = \\frac{1}{1 + H(p, q)}.\n",
    "$$\n",
    "\n",
    "In the cell below, we prepare everything we need to perform similarity queries based on the Hellinger distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae64cc48-fe8c-498b-86a1-3c678be82b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that returns similarities based on the Hellinger distance.\n",
    "\n",
    "from gensim import matutils\n",
    "import pandas as pd\n",
    "\n",
    "# Make a list of all the author-topic distributions.\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    '''Get similarity between two vectors'''\n",
    "    dist = matutils.hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n",
    "                              matutils.sparse2full(vec2, model.num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec):\n",
    "    '''Get similarity of vector to all authors.'''\n",
    "    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n",
    "    return sims\n",
    "\n",
    "def get_table(name, top_n=10, smallest_author=1):\n",
    "    '''\n",
    "    Get table with similarities, author names, and author sizes.\n",
    "    Return `top_n` authors as a dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get similarities.\n",
    "    sims = get_sims(model.get_author_topics(name))\n",
    "\n",
    "    # Arrange author names, similarities, and author sizes in a list of tuples.\n",
    "    table = []\n",
    "    for elem in enumerate(sims):\n",
    "        author_name = model.id2author[elem[0]]\n",
    "        sim = elem[1]\n",
    "        author_size = len(model.author2doc[author_name])\n",
    "        if author_size >= smallest_author:\n",
    "            table.append((author_name, sim, author_size))\n",
    "            \n",
    "    # Make dataframe and retrieve top authors.\n",
    "    df = pd.DataFrame(table, columns=['Author', 'Score', 'Size'])\n",
    "    df = df.sort_values('Score', ascending=False)[:top_n]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce677bc-47da-4d8b-b944-247cfc611bec",
   "metadata": {},
   "source": [
    "Now we can find the most similar authors to some particular author. We use the Pandas library to print the results in a nice looking tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62f4763e-e64b-40e5-98ed-e5c8356d1716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Score</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>YannLeCun</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>D.Henderson</td>\n",
       "      <td>0.973279</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>J.S.Denker</td>\n",
       "      <td>0.963552</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>MosfeqRashid</td>\n",
       "      <td>0.888570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>KazukiJoe</td>\n",
       "      <td>0.880607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>ClayD.Spence</td>\n",
       "      <td>0.877365</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>RalphWolf</td>\n",
       "      <td>0.872856</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>DavidChapman</td>\n",
       "      <td>0.859226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>EduardSackinger</td>\n",
       "      <td>0.840577</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>JamesPittman</td>\n",
       "      <td>0.816509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Author     Score  Size\n",
       "2422        YannLeCun  1.000000    11\n",
       "427       D.Henderson  0.973279     4\n",
       "986        J.S.Denker  0.963552     3\n",
       "1608     MosfeqRashid  0.888570     2\n",
       "1253        KazukiJoe  0.880607     1\n",
       "401      ClayD.Spence  0.877365     5\n",
       "1852        RalphWolf  0.872856     1\n",
       "492      DavidChapman  0.859226     1\n",
       "612   EduardSackinger  0.840577     3\n",
       "1041     JamesPittman  0.816509     1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table('YannLeCun')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314c361-cd13-4840-b71b-9822e279df2c",
   "metadata": {},
   "source": [
    "As before, we can specify the minimum author size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3061856-b9b1-45af-ac7b-0782f2397560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Score</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>JamesM.Bower</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>MatthewA.Wilson</td>\n",
       "      <td>0.910005</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>EveMarder</td>\n",
       "      <td>0.888783</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>K.Obermayer</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>RonKeesing</td>\n",
       "      <td>0.848466</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>K.Schulten</td>\n",
       "      <td>0.848361</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A.E.Friedman</td>\n",
       "      <td>0.848358</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>L.F.Abbott</td>\n",
       "      <td>0.848352</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>EytanRuppin</td>\n",
       "      <td>0.848342</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>LeifH.Finkel</td>\n",
       "      <td>0.817448</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author     Score  Size\n",
       "118     JamesM.Bower  1.000000    10\n",
       "182  MatthewA.Wilson  0.910005     3\n",
       "82         EveMarder  0.888783     3\n",
       "150      K.Obermayer  0.848624     3\n",
       "232       RonKeesing  0.848466     3\n",
       "151       K.Schulten  0.848361     3\n",
       "1       A.E.Friedman  0.848358     3\n",
       "157       L.F.Abbott  0.848352     4\n",
       "83       EytanRuppin  0.848342    11\n",
       "162     LeifH.Finkel  0.817448     3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table('JamesM.Bower', smallest_author=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf4c03-0454-420a-b530-90140f6793f8",
   "metadata": {},
   "source": [
    "### Serialized corpora\n",
    "\n",
    "The `AuthorTopicModel` class accepts serialized corpora, that is, corpora that are stored on the hard-drive rather than in memory. This is usually done when the corpus is too big to fit in memory. There are, however, some caveats to this functionality, which we will discuss here. As these caveats make this functionality less than ideal, it may be improved in the future.\n",
    "\n",
    "It is not necessary to read this section if you don't intend to use serialized corpora.\n",
    "\n",
    "In the following, an explanation, followed by an example and a summarization will be given.\n",
    "\n",
    "If the corpus is serialized, the user must specify `serialized=True`. Any input corpus can then be any type of iterable or generator.\n",
    "\n",
    "The model will then take the input corpus and serialize it in the `MmCorpus` format, which is [supported in Gensim](https://radimrehurek.com/gensim/corpora/mmcorpus.html).\n",
    "\n",
    "The user must specify the path where the model should serialize all input documents, for example `serialization_path='/tmp/model_serializer.mm'`. To avoid accidentally overwriting some important data, the model will raise an error if there already exists a file at `serialization_path`; in this case, either choose another path, or delete the old file.\n",
    "\n",
    "When you want to train on new data, and call `model.update(corpus, author2doc)`, all the old data and the new data have to be re-serialized. This can of course be quite computationally demanding, so it is recommended that you do this *only* when necessary; that is, wait until you have as much new data as possible to update, rather than updating the model for every new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c95bacdc-f10e-455c-8cce-3307904f0cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.14 s, sys: 136 ms, total: 4.28 s\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%time model_ser = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                               author2doc=author2doc, random_state=1, serialized=True, \\\n",
    "                               serialization_path='./tmp/model_serialization.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cad5eed-858e-4146-ab2c-506dbe58681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the file, once you're done using it.\n",
    "import os\n",
    "os.remove('./tmp/model_serialization.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d043fef-6c71-41e9-9270-85c1bcdc34be",
   "metadata": {},
   "source": [
    "In summary, when using serialized corpora:\n",
    "* Set `serialized=True`.\n",
    "* Set `serialization_path` to a path that doesn't already contain a file.\n",
    "* Wait until you have lots of data before you call `model.update(corpus, author2doc)`.\n",
    "* When done, delete the file at `serialization_path` if it's not needed anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4b836-dfe7-4d19-9283-89b062ae63c6",
   "metadata": {},
   "source": [
    "## What to try next\n",
    "\n",
    "Try the model on one of the datasets in the [StackExchange data dump](https://archive.org/details/stackexchange). You can treat the tags on the posts as authors and train a \"tag-topic\" model. There are many different categories, from statistics to cooking to philosophy, so you can pick on that you like. You can even try your hand at a [Kaggle competition](https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags) that uses tags in this dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Anaconda 2024.02)",
   "language": "python",
   "name": "anaconda-2024.02-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
